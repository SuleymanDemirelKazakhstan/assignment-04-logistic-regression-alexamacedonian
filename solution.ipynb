{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","import pandas as pd\n","import re\n","from nltk.stem import WordNetLemmatizer\n","from nltk.util import ngrams\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, recall_score\n","\n","nltk.download('wordnet', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","df = pd.read_csv('/Users/bbnv/Desktop/NLP/assignment4/sdu-inf376-2024-imdb-lr/train.csv')\n","\n","disasterTweets = df[df['sentiment'] == 'positive'].copy()\n","normalTweets = df[df['sentiment'] == 'negative'].copy()\n","\n","stop_words = set(nltk.corpus.stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def clean_and_lemmatize_text(text):\n","    text = re.sub(r'@[^\\s]+', '', text)  \n","    text = re.sub(r'[^\\w\\s]', '', text)  \n","    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n","    return text\n","\n","disasterTweets.loc[:, 'review'] = disasterTweets['review'].apply(clean_and_lemmatize_text)\n","normalTweets.loc[:, 'review'] = normalTweets['review'].apply(clean_and_lemmatize_text)\n","\n","disaster_word_counts = Counter()\n","normal_word_counts = Counter()\n","\n","for text in disasterTweets['review']:\n","    disaster_word_counts.update(text.split())\n","\n","for text in normalTweets['review']:\n","    normal_word_counts.update(text.split())\n","\n","# Find top 20 words by occurrence\n","top_disaster_words = disaster_word_counts.most_common(20)\n","top_normal_words = normal_word_counts.most_common(20)\n","\n","print(\"Top 20 words in disaster tweets:\")\n","for word, count in top_disaster_words:\n","    print(f\"{word}: {count}\")\n","\n","print(\"\\nTop 20 words in normal tweets:\")\n","for word, count in top_normal_words:\n","    print(f\"{word}: {count}\")\n","\n","combined_df = pd.concat([disasterTweets, normalTweets])\n","X = combined_df['review']\n","y = combined_df['sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","max_features_list = [100, 1000]\n","\n","for max_features in max_features_list:\n","    vectorizer = CountVectorizer(max_features=max_features)\n","    X_train_vec = vectorizer.fit_transform(X_train)\n","    X_test_vec = vectorizer.transform(X_test)\n","\n","    model = LogisticRegression()\n","    model.fit(X_train_vec, y_train)\n","\n","    y_pred = model.predict(X_test_vec)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred, pos_label='positive')\n","    recall = recall_score(y_test, y_pred, pos_label='positive')\n","\n","    print(f\"\\nResults for max_features = {max_features}:\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"F1-score: {f1:.4f}\")\n","    print(f\"Recall: {recall:.4f}\\n\")"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
